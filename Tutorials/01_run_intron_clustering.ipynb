{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's run intron clustering to annotate alternative splicing events given observed junctions in our cells "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LeafletSC \n",
    "import LeafletSC\n",
    "import os\n",
    "import pandas as pd \n",
    "\n",
    "# Define path that contains some junction files (only 2 files are used for this example, corresponding to 2 individual cells)\n",
    "juncs_path = \"/gpfs/commons/home/kisaev/LeafletSC/data/raw/junctions/\"\n",
    "print(\"The junctions are loaded from the following path: \" + juncs_path) \n",
    "\n",
    "# print the files in the path \n",
    "print(\"The files in the path are: \" + str(os.listdir(juncs_path)))\n",
    "\n",
    "# define path for saving the output data \n",
    "output_path = \"/gpfs/commons/home/kisaev/LeafletSC/data/processed/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first define some parameters for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LeafletSC.clustering.find_intron_clusters import main as find_intron_clusters\n",
    "from LeafletSC.clustering.prepare_model_input import main as prep_model_input\n",
    "from LeafletSC.clustering.find_intron_clusters import visualize_local_events\n",
    "\n",
    "# junc_files defines a path for where junction files can be found, in this case, the path is defined above\n",
    "junc_files = juncs_path\n",
    "\n",
    "# we provide a gtf file for the human genome as well to make better sense of the junctions that are detected in cells\n",
    "# please replace with the path to the gtf file on your system\n",
    "gtf_file=\"/gpfs/commons/groups/knowles_lab/Karin/genome_files/gencode.v43.basic.annotation.gtf\" \n",
    "\n",
    "# define additional parameters \n",
    "sequencing_type = \"single_cell\"\n",
    "\n",
    "# ensure output files are to be saved in output_path \n",
    "output_file = output_path + \"test_intron_clusters\"\n",
    "junc_bed_file= output_path + \"test_juncs.bed\" # you can load this file into IGV to visualize the junction coordinates \n",
    "min_intron_length = 50\n",
    "max_intron_length = 500000\n",
    "threshold_inc = 0.05 \n",
    "min_junc_reads = 2\n",
    "min_num_cells_wjunc = 2\n",
    "keep_singletons = False # ignore junctions that do not share splice sites with any other junction (likely const)\n",
    "junc_suffix = \"*.juncswbarcodes\" # depends on how you ran regtools "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run intron clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_juncs_df = find_intron_clusters(junc_files=junc_files, gtf_file=gtf_file, output_file=output_file, \n",
    "                       sequencing_type=sequencing_type, junc_bed_file=junc_bed_file, \n",
    "                       threshold_inc=threshold_inc, min_intron = min_intron_length,\n",
    "                       max_intron=max_intron_length, min_junc_reads=min_junc_reads,\n",
    "                       singleton=keep_singletons,\n",
    "                       junc_suffix=junc_suffix, min_num_cells_wjunc=min_num_cells_wjunc,\n",
    "                       run_notebook = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bed file \n",
    "juncs_bed = pd.read_csv(junc_bed_file, sep=\"\\t\", header=None)\n",
    "# remove columns 3 and 4 \n",
    "juncs_bed = juncs_bed.drop(columns=[3, 4])\n",
    "juncs_bed.columns = [\"Chromosome\", \"Start\", \"End\", \"Strand\", \"junction_id\", \"Start_b\", \"End_b\", \"gene_id\", \"gene_name\", \"transcript_id\", \"exon_id\"]\n",
    "\n",
    "# cmobine with all_juncs_df and prep df for visualization \n",
    "dat_vis = all_juncs_df[[\"chrom\", \"chromStart\", \"chromEnd\", \"strand\", \"intron_length\", \"counts_total\", \"junction_id\", \"Cluster\"]]\n",
    "dat_vis = dat_vis.drop_duplicates()\n",
    "# merge dat_vis with juncs_bed using all common columns \n",
    "dat_vis = dat_vis.merge(juncs_bed, how=\"left\", on=[\"junction_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from LeafletSC.clustering.find_intron_clusters import visualize_junctions\n",
    "junc_id = all_juncs_df.junction_id.sample(1).values[0]\n",
    "j = junc_id\n",
    "visualize_local_events(dat_vis, j, p_usage_ratio=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's convert the intron clusters to a format that can be used by LeafletSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intron_clusters = \"/gpfs/commons/home/kisaev/LeafletSC/data/processed/test_intron_clusters_50_500000_2_20240309_single_cell.gz\" # path to the intron clusters file\n",
    "output_file = output_path + \"test_model_input\" # name of the output file\n",
    "has_genes = \"yes\" # since we used a gtf file to obtain the intron clusters, we can set this to yes\n",
    "chunk_size = 5000 # number of junctions to process at a time from the intron clusters files\n",
    "metadata = None # can replace with path, if metadata is available for cells (cell type, origin, library ID...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_model_input(intron_clusters, output_file, has_genes, chunk_size, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a quick look at the input file that will go into the model to get familiarized with all the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input_data = \"/gpfs/commons/home/kisaev/LeafletSC/data/processed/test_model_input.h5\"\n",
    "summarized_data = pd.read_hdf(model_input_data, 'df')\n",
    "print(summarized_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that fow now, the values in cell_type default to the cell's path, in the future it will be possible to specify the cell type in the metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_data.cell_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see all the columns in the summarized data\n",
    "print(summarized_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can quickly visualize the overall junction usage ratio distribution across all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summarized_data.junc_ratio.hist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have everything need to run the Leaflet mixture model! Please refer to the next notebook for the next steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leafcutter-sc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
